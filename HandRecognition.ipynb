{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HandRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sripriya07/Hand-Recognition/blob/master/HandRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FTujr_11xsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6NBuzil2dm7",
        "colab_type": "text"
      },
      "source": [
        "# Hand Recognition\n",
        "\n",
        "The goal of this project is to train a Machine Learning algorithm capable of classifying images of different hand gestures, such as a fist, palm, showing the thumb, and others. With this, I'll be able to understand more about this field and create my own program that fits the data that I have. This particular classification problem can be useful for Gesture Navigation, for example. The method I'll be using is Deep Learning.\n",
        "\n",
        "Deep Learning is part of a broader family of machine learning methods. It is based on the use of layers that process the input data, extracting features from them and producing a mathematical model. The creation of this said 'model' will be more clear in the next session. In this specific project, we'll be aiming to classify different images of hand gestures, which means that the computer will have to \"learn\" the features of each gesture and classify them correctly. For example, if it is given an image of a hand doing a thumbs up gesture, the output of the model needs to be \"the hand is doing a thumbs up gesture\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnAIpHgp2grB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5aee1fb2-d894-424c-e388-b0189fc821ad"
      },
      "source": [
        "# Here we import everything we need for the project\n",
        "\n",
        "%matplotlib inline\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split # Helps with organizing data for training\n",
        "from sklearn.metrics import confusion_matrix # Helps present results as a confusion-matrix\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRsgQCma2nRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1cXiX3r2vq_",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data\n",
        "This project uses the Hand Gesture Recognition Database (link below) available on Kaggle. It contains 20000 images with different hands and hand gestures. There is a total of 10 hand gestures of 10 different people presented in the dataset. There are 5 female subjects and 5 male subjects. The images were captured using the Leap Motion hand tracking device.\n",
        "\n",
        "https://www.kaggle.com/gti-upm/leapgestrecog/version/1\n",
        "\n",
        "Hand Gesture  and  Label used :\n",
        "\n",
        "Thumb down\t-   0,\n",
        "Palm (Horizontal)-  1,\n",
        "L\t-   2, \n",
        "Fist (Horizontal) -\t3,\n",
        "Fist (Vertical)\t-4,\n",
        "Thumbs up -\t5,\n",
        "Index\t-6,\n",
        "OK\t-7,\n",
        "Palm (Vertical)-\t8,\n",
        "C-\t9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKLLifWt4Lc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip images, ignore this cell if files are already in the workspace\n",
        "!unzip leapGestRecog.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYYLUaJ54O5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to get all the paths for the images to later load them\n",
        "imagepaths = []\n",
        "\n",
        "# Go through all the files and subdirectories inside a folder and save path to images inside list\n",
        "for root, dirs, files in os.walk(\".\", topdown=False): \n",
        "  for name in files:\n",
        "    path = os.path.join(root, name)\n",
        "    if path.endswith(\"png\"): # We want only the images\n",
        "      imagepaths.append(path)\n",
        "\n",
        "print(len(imagepaths)) # If > 0, then a PNG image was loaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUE9zeJX4Sy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is used more for debugging and showing results later. It plots the image into the notebook\n",
        "\n",
        "def plot_image(path):\n",
        "  img = cv2.imread(path) # Reads the image into a numpy.array\n",
        "  img_cvt = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (RGB)\n",
        "  print(img_cvt.shape) # Prints the shape of the image just to check\n",
        "  plt.grid(False) # Without grid so we can see better\n",
        "  plt.imshow(img_cvt) # Shows the image\n",
        "  plt.xlabel(\"Width\")\n",
        "  plt.ylabel(\"Height\")\n",
        "  plt.title(\"Image \" + path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn9kX3gs4XYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_image(imagepaths[0]) #We plot the first image from our imagepaths array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYTuKPsi4XnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = [] # Image data\n",
        "y = [] # Labels\n",
        "\n",
        "# Loops through imagepaths to load images and labels into arrays\n",
        "for path in imagepaths:\n",
        "  img = cv2.imread(path) # Reads image and returns np.array\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY)\n",
        "  img = cv2.resize(img, (320, 120)) # Reduce image size so training can be faster\n",
        "  X.append(img)\n",
        "  \n",
        "  # Processing label in image path\n",
        "  category = path.split(\"/\")[3]\n",
        "  label = int(category.split(\"_\")[0][1]) # We need to convert 10_down to 00_down, or else it crashes\n",
        "  y.append(label)\n",
        "\n",
        "# Turn X and y into np.array to speed up train_test_split\n",
        "X = np.array(X, dtype=\"uint8\")\n",
        "X = X.reshape(len(imagepaths), 120, 320, 1) # Needed to reshape so CNN knows it's different images\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"Images loaded: \", len(X))\n",
        "print(\"Labels loaded: \", len(y))\n",
        "\n",
        "print(y[0], imagepaths[0]) # Debugging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-XoPEzU4lH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ts = 0.3 # Percentage of images that we want to use for testing. The rest is used for training.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukw4mkR34lXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muqXf_Rn4ufM",
        "colab_type": "text"
      },
      "source": [
        "# Creating model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2sruYeF4lZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recreate the exact same model, including weights and optimizer.\n",
        "# model = keras.models.load_model('handrecognition_model.h5')\n",
        "# model.summary()\n",
        "\n",
        "# To use the pre-trained model, just load it and skip to the next session."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpECA3IZ4ldc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import of keras model and hidden layers for our convolutional network\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv1qRk6r5VYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construction of model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 320, 1))) \n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu')) \n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuSyXfPv5VaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configures the model for training\n",
        "model.compile(optimizer='adam', # Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function.\n",
        "              loss='sparse_categorical_crossentropy', # Loss function, which tells us how bad our predictions are.\n",
        "              metrics=['accuracy']) # List of metrics to be evaluated by the model during training and testing."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEMUUffm5VeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trains the model for a given number of epochs (iterations on a dataset) and validates it.\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=2, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAc3yEQO5VhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save entire model to a HDF5 file\n",
        "model.save('handrecognition_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-MOwNYz5lRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n2iXQUs5ppY",
        "colab_type": "text"
      },
      "source": [
        "# Testing Model\n",
        "\n",
        "Now that we have the model compiled and trained, we need to check if it's good. First, we run model.evaluate to test the accuracy. Then, we make predictions and plot the images as long with the predicted labels and true labels to check everything. With that, we can see how our algorithm is working.\n",
        "Later, we produce a confusion matrix, which is a specific table layout that allows visualization of the performance of an algorithm.\n",
        "\n",
        "Overview:\n",
        "\n",
        "\n",
        "*   Evaluate model\n",
        "*   Pedictions\n",
        "*   Plot images with predictions\n",
        "*   Visualize model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DdL_atd5lT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test accuracy: {:2.2f}%'.format(test_acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYd0UTNx5lYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_test) # Make predictions towards the test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTzAvGrJ6QX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.argmax(predictions[0]), y_test[0] # If same, got it right"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSj2vMQ16TQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to plot images and labels for validation purposes\n",
        "def validate_9_images(predictions_array, true_label_array, img_array):\n",
        "  # Array for pretty printing and then figure size\n",
        "  class_names = [\"down\", \"palm\", \"l\", \"fist\", \"fist_moved\", \"thumb\", \"index\", \"ok\", \"palm_moved\", \"c\"] \n",
        "  plt.figure(figsize=(15,5))\n",
        "  \n",
        "  for i in range(1, 10):\n",
        "    # Just assigning variables\n",
        "    prediction = predictions_array[i]\n",
        "    true_label = true_label_array[i]\n",
        "    img = img_array[i]\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "    \n",
        "    # Plot in a good way\n",
        "    plt.subplot(3,3,i)\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "    predicted_label = np.argmax(prediction) # Get index of the predicted label from prediction\n",
        "    \n",
        "    # Change color of title based on good prediction or not\n",
        "    if predicted_label == true_label:\n",
        "      color = 'blue'\n",
        "    else:\n",
        "      color = 'red'\n",
        "\n",
        "    plt.xlabel(\"Predicted: {} {:2.0f}% (True: {})\".format(class_names[predicted_label],\n",
        "                                  100*np.max(prediction),\n",
        "                                  class_names[true_label]),\n",
        "                                  color=color)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do-8tlwp6Yh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validate_9_images(predictions, y_test, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guSSryOb6Y8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = np.argmax(predictions, axis=1) # Transform predictions into 1-D array with label number"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6UAfaBu6gCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# H = Horizontal\n",
        "# V = Vertical\n",
        "\n",
        "pd.DataFrame(confusion_matrix(y_test, y_pred), \n",
        "             columns=[\"Predicted Thumb Down\", \"Predicted Palm (H)\", \"Predicted L\", \"Predicted Fist (H)\", \"Predicted Fist (V)\", \"Predicted Thumbs up\", \"Predicted Index\", \"Predicted OK\", \"Predicted Palm (V)\", \"Predicted C\"],\n",
        "             index=[\"Actual Thumb Down\", \"Actual Palm (H)\", \"Actual L\", \"Actual Fist (H)\", \"Actual Fist (V)\", \"Actual Thumbs up\", \"Actual Index\", \"Actual OK\", \"Actual Palm (V)\", \"Actual C\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zmiQQxb6gSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}